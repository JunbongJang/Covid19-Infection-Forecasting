{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:46:16.569329Z",
     "start_time": "2020-12-07T17:46:14.545879Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import datetime\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import theano\n",
    "theano.config.floatX = 'float64'  # necessary to prevent an dtype error\n",
    "import theano.tensor as tt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.SEIR_Forecast.Bayesian_Inference_plotting import *\n",
    "import src.SEIR_Forecast.Bayesian_Inference_SEIR_helper as model_helper\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T15:31:33.753157Z",
     "start_time": "2020-12-07T15:31:33.749984Z"
    }
   },
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:29:54.256518Z",
     "start_time": "2020-12-07T17:29:51.181355Z"
    }
   },
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "with model:\n",
    "    mu1 = pm.Normal(\"mu1\", mu=0, sigma=1)\n",
    "    trace = pm.sample(2000, init=None, step=pm.Metropolis())\n",
    "    print(trace.stat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:29:54.262977Z",
     "start_time": "2020-12-07T17:29:54.258596Z"
    }
   },
   "outputs": [],
   "source": [
    "varnames = [str(x).replace('_log__', '') for x in model.free_RVs]\n",
    "print(varnames)\n",
    "print(trace['mu1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:29:54.586255Z",
     "start_time": "2020-12-07T17:29:54.264535Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist_custom(model, trace, varname, colors = ('tab:blue', 'tab:green'), bins = 50):\n",
    "    \"\"\"\n",
    "    Plots one histogram of the prior and posterior distribution of the variable varname.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: pm.Model instance\n",
    "    trace: trace of the model\n",
    "    ax: matplotlib.axes  instance\n",
    "    varname: string\n",
    "    colors: list with 2 colornames\n",
    "    bins:  number or array\n",
    "        passed to np.hist\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.hist(trace[varname], bins=bins, density=True, color=colors[1],\n",
    "            label='Posterior')\n",
    "    ax = plt.gca()\n",
    "    limits = ax.get_xlim()\n",
    "    x = np.linspace(*limits, num=100)\n",
    "    try:\n",
    "        ax.plot(x, get_prior_distribution(model, x, varname), label='Prior',\n",
    "                color=colors[0], linewidth=3)\n",
    "    except:\n",
    "        pass\n",
    "    ax.set_xlim(*limits)\n",
    "    ax.set_xlabel(varname)\n",
    "    \n",
    "plot_hist_custom(model, trace, 'mu1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian MCMC main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:46:38.158726Z",
     "start_time": "2020-12-07T17:46:38.128489Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_change_points(final_date, final_change_date, cluster_id):\n",
    "    print('get_change_points', final_date, cluster_id)\n",
    "\n",
    "    if final_date == datetime.datetime(2020, 4, 30):\n",
    "        prior_date_1 = datetime.datetime(2020, 3, 25)\n",
    "        prior_date_2 = datetime.datetime(2020, 4, 1)\n",
    "        prior_date_3 = datetime.datetime(2020, 4, 8)\n",
    "            \n",
    "    elif final_date == datetime.datetime(2020, 5, 15):\n",
    "        prior_date_1 = datetime.datetime(2020, 4, 10)\n",
    "        prior_date_2 = datetime.datetime(2020, 4, 17)\n",
    "        prior_date_3 = datetime.datetime(2020, 4, 24)\n",
    "    \n",
    "    elif final_date == datetime.datetime(2020, 5, 31):\n",
    "        prior_date_1 = datetime.datetime(2020, 4, 25)\n",
    "        prior_date_2 = datetime.datetime(2020, 5, 1)\n",
    "        prior_date_3 = datetime.datetime(2020, 5, 8)\n",
    "        \n",
    "    elif final_date == datetime.datetime(2020, 6, 15):\n",
    "        prior_date_1 = datetime.datetime(2020, 5, 10)\n",
    "        prior_date_2 = datetime.datetime(2020, 5, 17)\n",
    "        prior_date_3 = datetime.datetime(2020, 5, 24)\n",
    "            \n",
    "    elif final_date == datetime.datetime(2020, 6, 30):\n",
    "        prior_date_1 = datetime.datetime(2020, 5, 25)\n",
    "        prior_date_2 = datetime.datetime(2020, 6, 1)\n",
    "        prior_date_3 = datetime.datetime(2020, 6, 8)\n",
    "            \n",
    "        \n",
    "    change_points = [dict(pr_mean_date_begin_transient=prior_date_1,\n",
    "                      pr_sigma_date_begin_transient=1,\n",
    "                      pr_median_lambda=0.2,\n",
    "                      pr_sigma_lambda=0.5),\n",
    "                 dict(pr_mean_date_begin_transient=prior_date_2,\n",
    "                      pr_sigma_date_begin_transient=1,\n",
    "                      pr_median_lambda=0.2,\n",
    "                      pr_sigma_lambda=0.5),\n",
    "                 dict(pr_mean_date_begin_transient=prior_date_3,\n",
    "                      pr_sigma_date_begin_transient=1,\n",
    "                      pr_median_lambda=0.2,\n",
    "                      pr_sigma_lambda=0.5),\n",
    "                 dict(pr_mean_date_begin_transient=final_change_date,\n",
    "                      pr_sigma_date_begin_transient=1,\n",
    "                      pr_median_lambda=0.2,\n",
    "                      pr_sigma_lambda=0.5)]\n",
    "                      \n",
    "                      \n",
    "    return change_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T17:46:39.106870Z",
     "start_time": "2020-12-07T17:46:38.820282Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(sir_model, N_SAMPLES, cluster_save_path):\n",
    "    print('sample start')\n",
    "    with sir_model:\n",
    "        trace = pm.sample(N_SAMPLES, model=sir_model, step=pm.Metropolis(), progressbar=True)\n",
    "    pm.save_trace(trace, cluster_save_path + 'sir_model.trace', overwrite=True)\n",
    "    print('sample end')\n",
    "    # -------- prepare data for visualization ---------------\n",
    "    varnames = get_all_free_RVs_names(sir_model)\n",
    "    #for varname in varnames:\n",
    "        #visualize_trace(trace[varname][:, None], varname, N_SAMPLES)\n",
    "        \n",
    "    lambda_t = np.median(trace['lambda_t'][:, :], axis=0)\n",
    "    μ = np.median(trace['mu'][:, None], axis=0)\n",
    "\n",
    "    # -------- visualize histogram ---------------\n",
    "    num_cols = 5\n",
    "    num_rows = int(np.ceil(len(varnames)/num_cols))\n",
    "    x_size = num_cols * 2.5\n",
    "    y_size = num_rows * 2.5\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize = (x_size, y_size),squeeze=False)\n",
    "    i_ax = 0\n",
    "    for i_row, axes_row in enumerate(axes):\n",
    "        for i_col, ax in enumerate(axes_row):\n",
    "            if i_ax >= len(varnames):\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "            else:\n",
    "                plot_hist(sir_model, trace, ax, varnames[i_ax], \n",
    "                                         colors=('tab:blue', 'tab:green'))\n",
    "            if i_col == 0:\n",
    "                ax.set_ylabel('Density')\n",
    "            if i_col == 0 and i_row == 0:\n",
    "                ax.legend()\n",
    "            i_ax += 1\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.4)\n",
    "    plt.savefig(cluster_save_path + 'plot_hist.png')\n",
    "    plt.clf()\n",
    "\n",
    "    np.save(cluster_save_path + 'varnames.npy', varnames)\n",
    "    np.save(cluster_save_path + 'SIR_params.npy', [lambda_t, μ])\n",
    "    \n",
    "    \n",
    "\n",
    "def SIR_with_change_points(\n",
    "    S_begin_beta,\n",
    "    I_begin_beta,\n",
    "    new_cases_obs,\n",
    "    change_points_list,\n",
    "    date_begin_simulation,\n",
    "    num_days_sim,\n",
    "    diff_data_sim,\n",
    "    N,\n",
    "    priors_dict=None,\n",
    "    weekends_modulated=False\n",
    "):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_cases_obs : list or array\n",
    "            Timeseries (day over day) of newly reported cases (not the total number)\n",
    "        change_points_list : list of dicts\n",
    "            List of dictionaries, each corresponding to one change point.\n",
    "            Each dict can have the following key-value pairs. If a pair is not provided,\n",
    "            the respective default is used.\n",
    "                * pr_mean_date_begin_transient :     datetime.datetime, NO default\n",
    "                * pr_median_lambda :                 number, same as default priors, below\n",
    "                * pr_sigma_lambda :                  number, same as default priors, below\n",
    "                * pr_sigma_date_begin_transient :    number, 3\n",
    "                * pr_median_transient_len :          number, 3\n",
    "                * pr_sigma_transient_len :           number, 0.3\n",
    "        date_begin_simulation: datetime.datetime\n",
    "            The begin of the simulation data\n",
    "        num_days_sim : integer\n",
    "            Number of days to forecast into the future\n",
    "        diff_data_sim : integer\n",
    "            Number of days that the simulation-begin predates the first data point in\n",
    "            `new_cases_obs`. This is necessary so the model can fit the reporting delay.\n",
    "            Set this parameter to a value larger than what you expect to find\n",
    "            for the reporting delay.\n",
    "            should be significantly larger than the expected delay,\n",
    "            in order to always fit the same number of data points.\n",
    "        N : number\n",
    "            The population size. For Germany, we used 83e6\n",
    "        priors_dict : dict\n",
    "            Dictionary of the prior assumptions\n",
    "            Possible key-value pairs (and default values) are:\n",
    "                * pr_beta_I_begin :        number, default = 100\n",
    "                * pr_median_lambda_0 :     number, default = 0.4\n",
    "                * pr_sigma_lambda_0 :      number, default = 0.5\n",
    "                * pr_median_mu :           number, default = 1/8\n",
    "                * pr_sigma_mu :            number, default = 0.2\n",
    "                * pr_median_delay :        number, default = 8\n",
    "                * pr_sigma_delay :         number, default = 0.2\n",
    "                * pr_beta_sigma_obs :      number, default = 10\n",
    "                * week_end_days :          tuple,  default = (6,7)\n",
    "                * pr_mean_weekend_factor : number, default = 0.7\n",
    "                * pr_sigma_weekend_factor :number, default = 0.17\n",
    "        weekends_modulated : bool\n",
    "            Whether to add the prior that cases are less reported on week ends. Multiplies the new cases numbers on weekends\n",
    "            by a number between 0 and 1, given by a prior beta distribution. The beta distribution is parametrised\n",
    "            by pr_mean_weekend_factor and pr_sigma_weekend_factor\n",
    "        weekend_modulation_type : 'step' or 'abs_sine':\n",
    "            whether the weekends are modulated by a step function, which only multiplies the days given by  week_end_days\n",
    "            by the week_end_factor, or whether the whole week is modulated by an abs(sin(x)) function, with an offset\n",
    "            with flat prior.\n",
    "        Returns\n",
    "        -------\n",
    "        : pymc3.Model\n",
    "            Returns an instance of pymc3 model with the change points\n",
    "    \"\"\"\n",
    "    if priors_dict is None:\n",
    "        priors_dict = dict()\n",
    "\n",
    "    default_priors = dict(\n",
    "        pr_beta_I_begin=10000.0,\n",
    "        pr_median_lambda_0=0.2,\n",
    "        pr_sigma_lambda_0=0.5,\n",
    "        pr_median_mu=1 / 8,\n",
    "        pr_sigma_mu=0.2,\n",
    "        pr_median_delay= 1.0, # 1.0,\n",
    "        pr_sigma_delay=0.2,\n",
    "        pr_beta_sigma_obs=5.0,\n",
    "        week_end_days = (6,7),\n",
    "        pr_mean_weekend_factor=0.7,\n",
    "        pr_sigma_weekend_factor=0.17\n",
    "    )\n",
    "    default_priors_change_points = dict(\n",
    "        pr_median_lambda=default_priors[\"pr_median_lambda_0\"],\n",
    "        pr_sigma_lambda=default_priors[\"pr_sigma_lambda_0\"],\n",
    "        pr_sigma_date_begin_transient=3.0,\n",
    "        pr_median_transient_len=3.0,\n",
    "        pr_sigma_transient_len=0.3,\n",
    "        pr_mean_date_begin_transient=None,\n",
    "    )\n",
    "\n",
    "    if not weekends_modulated:\n",
    "        del default_priors['week_end_days']\n",
    "        del default_priors['pr_mean_weekend_factor']\n",
    "        del default_priors['pr_sigma_weekend_factor']\n",
    "\n",
    "    for prior_name in priors_dict.keys():\n",
    "        if prior_name not in default_priors:\n",
    "            raise RuntimeError(f\"Prior with name {prior_name} not known\")\n",
    "    for change_point in change_points_list:\n",
    "        for prior_name in change_point.keys():\n",
    "            if prior_name not in default_priors_change_points:\n",
    "                raise RuntimeError(f\"Prior with name {prior_name} not known\")\n",
    "\n",
    "    for prior_name, value in default_priors.items():\n",
    "        if prior_name not in priors_dict:\n",
    "            priors_dict[prior_name] = value\n",
    "            # print(f\"{prior_name} was set to default value {value}\")\n",
    "    for prior_name, value in default_priors_change_points.items():\n",
    "        for i_cp, change_point in enumerate(change_points_list):\n",
    "            if prior_name not in change_point:\n",
    "                change_point[prior_name] = value\n",
    "                # print(f\"{prior_name} of change point {i_cp} was set to default value {value}\")\n",
    "\n",
    "    if num_days_sim < len(new_cases_obs) + diff_data_sim:\n",
    "        raise RuntimeError(\n",
    "            \"Simulation ends before the end of the data. Increase num_days_sim.\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------------------ #\n",
    "    # Model and prior implementation\n",
    "    # ------------------------------------------------------------------------------ #\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # all pm functions now apply on the model instance\n",
    "        # true cases at begin of loaded data but we do not know the real number\n",
    "        I_begin = pm.Normal(name=\"I_begin\", mu=I_begin_beta, sigma=I_begin_beta/10)\n",
    "        S_begin = pm.Normal(name=\"S_begin\", mu=S_begin_beta, sigma=S_begin_beta/10)\n",
    "        # S_begin = N - I_begin\n",
    "\n",
    "        # I_begin_print = tt.printing.Print('I_begin')(I_begin)\n",
    "        # S_begin_print = tt.printing.Print('S_begin')(S_begin)\n",
    "        # fraction of people that are newly infected each day\n",
    "        lambda_list = []\n",
    "        lambda_list.append(\n",
    "            pm.Lognormal(\n",
    "                name=\"lambda_0\",\n",
    "                mu=np.log(priors_dict[\"pr_median_lambda_0\"]),\n",
    "                sigma=priors_dict[\"pr_sigma_lambda_0\"],\n",
    "            )\n",
    "        )\n",
    "        for i, cp in enumerate(change_points_list):\n",
    "            lambda_list.append(\n",
    "                pm.Lognormal(\n",
    "                    name=f\"lambda_{i + 1}\",\n",
    "                    mu=np.log(cp[\"pr_median_lambda\"]),\n",
    "                    sigma=cp[\"pr_sigma_lambda\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # list of start dates of the transient periods of the change points\n",
    "        tr_begin_list = []\n",
    "        dt_before = date_begin_simulation\n",
    "        for i, cp in enumerate(change_points_list):\n",
    "            dt_begin_transient = cp[\"pr_mean_date_begin_transient\"]\n",
    "            if dt_before is not None and dt_before > dt_begin_transient:\n",
    "                raise RuntimeError(\"Dates of change points are not temporally ordered\")\n",
    "            print('--------------Hi----------')\n",
    "            print((dt_begin_transient - date_begin_simulation).days)\n",
    "            print(dt_begin_transient, date_begin_simulation)\n",
    "            prior_mean = (dt_begin_transient - date_begin_simulation).days - 1  # convert the provided date format (argument) into days (a number)\n",
    "\n",
    "            tr_begin = pm.Normal(\n",
    "                name=f\"transient_begin_{i}\",\n",
    "                mu=prior_mean,\n",
    "                sigma=cp[\"pr_sigma_date_begin_transient\"],\n",
    "            )\n",
    "            tr_begin_list.append(tr_begin)\n",
    "            dt_before = dt_begin_transient\n",
    "\n",
    "        # same for transient times\n",
    "        tr_len_list = []\n",
    "        for i, cp in enumerate(change_points_list):\n",
    "            tr_len = pm.Lognormal(\n",
    "                name=f\"transient_len_{i}\",\n",
    "                mu=np.log(cp[\"pr_median_transient_len\"]),\n",
    "                sigma=cp[\"pr_sigma_transient_len\"],\n",
    "            )\n",
    "            tr_len_list.append(tr_len)\n",
    "\n",
    "        # build the time-dependent spreading rate\n",
    "        lambda_t_list = [lambda_list[0] * tt.ones(num_days_sim)]\n",
    "        lambda_before = lambda_list[0]\n",
    "\n",
    "        for tr_begin, tr_len, lambda_after in zip(\n",
    "            tr_begin_list, tr_len_list, lambda_list[1:]\n",
    "        ):\n",
    "            lambda_t = model_helper.smooth_step_function(\n",
    "                start_val=0,\n",
    "                end_val=1,\n",
    "                t_begin=tr_begin,\n",
    "                t_end=tr_begin + tr_len,\n",
    "                t_total=num_days_sim,\n",
    "            ) * (lambda_after - lambda_before)\n",
    "            lambda_before = lambda_after\n",
    "            lambda_t_list.append(lambda_t)\n",
    "        lambda_t = sum(lambda_t_list)\n",
    "\n",
    "        # fraction of people that recover each day, recovery rate mu\n",
    "        mu = pm.Lognormal(\n",
    "            name=\"mu\",\n",
    "            mu=np.log(priors_dict[\"pr_median_mu\"]),\n",
    "            sigma=priors_dict[\"pr_sigma_mu\"],\n",
    "        )\n",
    "\n",
    "        # delay in days between contracting the disease and being recorded\n",
    "        delay = pm.Lognormal(\n",
    "            name=\"delay\",\n",
    "            mu=np.log(priors_dict[\"pr_median_delay\"]),\n",
    "            sigma=priors_dict[\"pr_sigma_delay\"],\n",
    "        )\n",
    "\n",
    "        # prior of the error of observed cases\n",
    "        sigma_obs = pm.HalfCauchy(\"sigma_obs\", beta=priors_dict[\"pr_beta_sigma_obs\"])\n",
    "\n",
    "        # -------------------------------------------------------------------------- #\n",
    "        # training the model with loaded data provided as argument\n",
    "        # -------------------------------------------------------------------------- #\n",
    "\n",
    "        S, I, new_I = _SIR_model(\n",
    "            lambda_t=lambda_t, mu=mu, S_begin=S_begin, I_begin=I_begin, N=N\n",
    "        )\n",
    "\n",
    "        # ignore this delay\n",
    "        # new_cases_inferred = model_helper.delay_cases(\n",
    "        #     new_I_t=new_I,\n",
    "        #     len_new_I_t=num_days_sim,\n",
    "        #     len_out=num_days_sim - diff_data_sim,\n",
    "        #     delay=delay,\n",
    "        #     delay_diff=diff_data_sim,\n",
    "        # )\n",
    "        new_cases_inferred = new_I\n",
    "\n",
    "        # likelihood of the model:\n",
    "        # observed cases are distributed following studentT around the model.\n",
    "        # we want to approximate a Poisson distribution of new cases.\n",
    "        # we choose nu=4 to get heavy tails and robustness to outliers.\n",
    "        # https://www.jstor.org/stable/2290063\n",
    "        num_days_data = new_cases_obs.shape[-1]\n",
    "        pm.StudentT(\n",
    "            name=\"_new_cases_studentT\",\n",
    "            nu=4,\n",
    "            mu=new_cases_inferred[:num_days_data],\n",
    "            sigma=tt.abs_(new_cases_inferred[:num_days_data] + 1) ** 0.5\n",
    "            * sigma_obs,  # +1 and tt.abs to avoid nans\n",
    "            observed=new_cases_obs,\n",
    "        )\n",
    "\n",
    "        # add these observables to the model so we can extract a time series of them\n",
    "        # later via e.g. `model.trace['lambda_t']`\n",
    "        pm.Deterministic(\"lambda_t\", lambda_t)\n",
    "        pm.Deterministic(\"new_cases\", new_cases_inferred)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def _SIR_model(lambda_t, mu, S_begin, I_begin, N):\n",
    "    \"\"\"\n",
    "        Implements the susceptible-infected-recovered model\n",
    "        Parameters\n",
    "        ----------\n",
    "        lambda_t : ~numpy.ndarray\n",
    "            time series of spreading rate, the length of the array sets the\n",
    "            number of steps to run the model for\n",
    "        mu : number\n",
    "            recovery rate\n",
    "        S_begin : number\n",
    "            initial number of susceptible at first time step\n",
    "        I_begin : number\n",
    "            initial number of infected\n",
    "        N : number\n",
    "            population size\n",
    "        Returns\n",
    "        -------\n",
    "        S : array\n",
    "            time series of the susceptible\n",
    "        I : array\n",
    "            time series of the infected\n",
    "        new_I : array\n",
    "            time series of the new infected\n",
    "    \"\"\"\n",
    "    new_I_0 = tt.zeros_like(I_begin)\n",
    "\n",
    "    def next_day(lambda_t, S_t, I_t, _, mu, N):\n",
    "        new_I_t = lambda_t / N * I_t * S_t\n",
    "        S_t = S_t - new_I_t\n",
    "        I_t = I_t + new_I_t - mu * I_t\n",
    "        I_t = tt.clip(I_t, 0, N)  # for stability\n",
    "        return S_t, I_t, new_I_t\n",
    "\n",
    "    # theano scan returns two tuples, first one containing a time series of\n",
    "    # what we give in outputs_info : S, I, new_I\n",
    "    outputs, _ = theano.scan(\n",
    "        fn=next_day,\n",
    "        sequences=[lambda_t],\n",
    "        outputs_info=[S_begin, I_begin, new_I_0],\n",
    "        non_sequences=[mu, N],\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-07T17:46:39.605Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Junbong Jang\n",
    "Date 4/29/2020\n",
    "\n",
    "Load timeseries data, train the model, and forecast\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from src.EDA.parseJohnsHopkins import johnsHopkinsPopulation, getTzuHsiClusters\n",
    "from src.SEIR_Forecast import Bayesian_Inference_SEIR\n",
    "from src.SEIR_Forecast.Bayesian_Inference_plotting import plot_cases\n",
    "from src.SEIR_Forecast.timeseries_eval import *\n",
    "from src.SEIR_Forecast.reassignment_helper import *\n",
    "from src.SEIR_Forecast.data_processing import *\n",
    "from src.SEIR_Forecast.visualizer import *\n",
    "from src.SEIR_Forecast.SIR import sir_forecast_a_county\n",
    "\n",
    "\n",
    "def forecast_main(clusters, cases_df, vel_cases_df, population_df, cluster_mode, init_cluster_num, max_cluster_num, initial_date,\n",
    "                  final_date, final_change_date, num_days_future, dataset_final_date, run_mode, root_save_path):\n",
    "    rmse_per_cluster_list = []\n",
    "    total_re_per_cluster_list = []\n",
    "    mean_rsquared_per_cluster_list = []\n",
    "    mape_per_cluster_list = []\n",
    "    wape_per_cluster_list = []\n",
    "\n",
    "    mse_per_county_per_cluster_list = []\n",
    "    re_per_county_per_cluster_list = []\n",
    "    rsquared_per_county_per_cluster_list = []\n",
    "    mape_per_county_per_cluster_list = []\n",
    "    wape_per_county_per_cluster_list = []\n",
    "\n",
    "    unclustered_rmse_per_cluster_list = []\n",
    "    unclustered_mse_per_county_per_cluster_list = []\n",
    "    unclustered_total_re_per_cluster_list = []\n",
    "    unclustered_re_per_county_per_cluster_list = []\n",
    "\n",
    "    initial_date = datetime.datetime.strptime(f'{initial_date}/2020', '%m/%d/%Y')\n",
    "    final_date = datetime.datetime.strptime(f'{final_date}/2020', '%m/%d/%Y')\n",
    "    dataset_final_date = datetime.datetime.strptime(f'{dataset_final_date}/2020', '%m/%d/%Y')\n",
    "\n",
    "    # cluster id 0 was not clustered by Tzu Hsi but I still use it\n",
    "    for cluster_id in range(init_cluster_num, max_cluster_num):\n",
    "        if cluster_mode == 'unclustered':\n",
    "            cluster_id = 'All'\n",
    "            chosen_cluster_series = clusters\n",
    "        else:\n",
    "            chosen_cluster_series = clusters[clusters == cluster_id]\n",
    "        cluster_counties = chosen_cluster_series.index.tolist()\n",
    "\n",
    "        print('-----------------------------')\n",
    "        print('Cluster ID: ', cluster_id)\n",
    "        # ------------- Create save folders --------------\n",
    "        cluster_save_path = root_save_path + f'/cluster_{cluster_id}/'\n",
    "        if os.path.isdir(cluster_save_path) is False:\n",
    "            os.mkdir(cluster_save_path)\n",
    "        cluster_all_save_path = root_save_path + f'/cluster_All/'\n",
    "\n",
    "        # -------------- Data Preprocessing --------------\n",
    "        cluster_cases_df, proc_population_series = preprocess_dataset(cases_df.copy(), population_df.copy(), cluster_counties)\n",
    "        cluster_vel_cases_df, _ = preprocess_dataset(vel_cases_df.copy(), population_df.copy(), cluster_counties)\n",
    "\n",
    "        cluster_cases_df, current_cumulative_cases_df, future_cumulative_cases_df, old_cumulative_infected_cases_series, date_begin_sim, num_days_sim = \\\n",
    "            process_date(cluster_cases_df, initial_date, final_date, dataset_final_date, num_days_future)\n",
    "        cluster_vel_cases_df, current_vel_cases_df, future_vel_cases_df, _, _, _ = \\\n",
    "            process_date(cluster_vel_cases_df, initial_date, final_date, dataset_final_date, num_days_future)\n",
    "\n",
    "\n",
    "        current_cumulative_cases_series = current_cumulative_cases_df.sum(axis=1)\n",
    "        current_vel_cases_series = current_vel_cases_df.sum(axis=1)\n",
    "        cluster_total_population = proc_population_series.sum()\n",
    "        future_cumulative_cases = future_cumulative_cases_df.sum(axis=1)[-1]\n",
    "\n",
    "        print('old_cumulative_infected_cases_series:', old_cumulative_infected_cases_series)\n",
    "        print('Cumulative future cases:', future_cumulative_cases)\n",
    "        print('population:', cluster_total_population)\n",
    "        print('Remaining population:', cluster_total_population - future_cumulative_cases)\n",
    "\n",
    "        visualize_trend_with_r_not(cluster_id, cluster_vel_cases_df, cluster_save_path)\n",
    "\n",
    "        # --------------- Get SIR Model -----------------\n",
    "        # convert cumulative infected to daily total infected cases\n",
    "        current_total_cases_series = current_cumulative_cases_series - old_cumulative_infected_cases_series.sum()\n",
    "        future_total_cases_df = future_cumulative_cases_df - old_cumulative_infected_cases_series\n",
    "\n",
    "        day_1_cumulative_infected_cases = current_cumulative_cases_series[0]\n",
    "        S_begin_beta = cluster_total_population - day_1_cumulative_infected_cases\n",
    "        I_begin_beta = current_total_cases_series[0]  # day 1 total infected cases\n",
    "\n",
    "        print('day_1_cumulative_infected_cases: ', day_1_cumulative_infected_cases)\n",
    "        print('S_begin_beta: ', S_begin_beta)\n",
    "        print('I_begin_beta: ', I_begin_beta)\n",
    "\n",
    "        change_points = get_change_points(final_date, final_change_date, cluster_id)\n",
    "#         print('change_points')\n",
    "#         print(change_points)\n",
    "#         sys.stderr.close()\n",
    "        sir_model = SIR_with_change_points(S_begin_beta,\n",
    "                                           I_begin_beta,\n",
    "                                           current_vel_cases_series.to_numpy(),  # current_total_cases_series.to_numpy(),\n",
    "                                           change_points_list=change_points,\n",
    "                                           date_begin_simulation=date_begin_sim,\n",
    "                                           num_days_sim=num_days_sim,\n",
    "                                           diff_data_sim=0,\n",
    "                                           N=cluster_total_population)\n",
    "\n",
    "        # ---------- Estimate Parameters for SIR model ------------\n",
    "        if run_mode == 'train':\n",
    "            run(sir_model, N_SAMPLES=500, cluster_save_path=cluster_save_path)\n",
    "\n",
    "        elif run_mode == 'eval':\n",
    "            trace = pm.load_trace(cluster_save_path + 'sir_model.trace', model=sir_model)\n",
    "            susceptible_series = proc_population_series - current_cumulative_cases_df.loc[final_date]\n",
    "\n",
    "            # ---------- Forecast using unclustered data ------------------\n",
    "            t = range(len(future_vel_cases_df.values))\n",
    "            if cluster_mode == 'clustered':\n",
    "                lambda_t, μ = np.load(cluster_all_save_path + 'SIR_params.npy', allow_pickle=True)\n",
    "                beta, gamma = lambda_t[-1], μ[0]\n",
    "                print('beta, gamma', beta, gamma)\n",
    "                cluster_all_vel_case_forecast = sir_forecast_a_county(susceptible_series.sum(), moving_average_from_df(current_vel_cases_df).sum(),\n",
    "                                                             cluster_total_population, t, beta, gamma, '', '')\n",
    "            else:\n",
    "                cluster_all_vel_case_forecast = None\n",
    "\n",
    "            # ----------- Forecast using clustered data ------------------\n",
    "            lambda_t, μ = np.load(cluster_save_path + 'SIR_params.npy', allow_pickle=True)\n",
    "            beta, gamma = lambda_t[-1], μ[0]\n",
    "            print('beta, gamma', beta, gamma)\n",
    "            cluster_forecast_I0 = np.mean(trace['new_cases'][:, len(current_vel_cases_series)], axis=0)\n",
    "\n",
    "            cluster_vel_case_forecast = sir_forecast_a_county(susceptible_series.sum(), cluster_forecast_I0,\n",
    "                                                         cluster_total_population, t, beta, gamma, '', '')\n",
    "\n",
    "            # ----------- Forecast Visualization ---------------\n",
    "            plot_cases(cluster_id, trace, current_vel_cases_series, future_vel_cases_df, cluster_vel_case_forecast,\n",
    "                    cluster_all_vel_case_forecast, date_begin_sim, diff_data_sim=0, num_days_future=num_days_future, cluster_save_path=cluster_save_path)\n",
    "\n",
    "            # ---------- Evaluation per county -----------\n",
    "            cluster_mse_dict, cluster_re_dict, cluster_rsquared_dict, cluster_mape_dict, cluster_wape_dict = \\\n",
    "                eval_per_cluster(susceptible_series, moving_average_from_df(current_vel_cases_df), future_vel_cases_df, proc_population_series, num_days_future,cluster_save_path)\n",
    "\n",
    "            if cluster_mode == 'unclustered':\n",
    "                for cluster_id in range(0, max_cluster_num):\n",
    "                    local_mse_list = []\n",
    "                    local_re_list = []\n",
    "                    chosen_cluster_series = clusters[clusters == cluster_id]\n",
    "                    cluster_counties = chosen_cluster_series.index.tolist()\n",
    "                    for a_county in cluster_counties:\n",
    "                        if a_county in cluster_mse_dict:\n",
    "                            local_mse_list.append(cluster_mse_dict[a_county])\n",
    "                        if a_county in cluster_re_dict:\n",
    "                            local_re_list.append(cluster_re_dict[a_county])\n",
    "                    unclustered_rmse_per_cluster_list.append(math.sqrt(statistics.mean(local_mse_list)))\n",
    "                    unclustered_mse_per_county_per_cluster_list.append(local_mse_list)\n",
    "                    unclustered_total_re_per_cluster_list.append(statistics.mean(local_re_list))\n",
    "                    unclustered_re_per_county_per_cluster_list.append(local_re_list)\n",
    "\n",
    "            elif cluster_mode == 'clustered':\n",
    "                rmse_per_cluster_list.append(math.sqrt(statistics.mean(cluster_mse_dict.values())))\n",
    "                total_re_per_cluster_list.append(statistics.mean(cluster_re_dict.values()))\n",
    "                mean_rsquared_per_cluster_list.append(statistics.mean(cluster_rsquared_dict.values()))\n",
    "                # mape_per_cluster_list.append(statistics.mean(cluster_mape_dict.values()))\n",
    "                wape_per_cluster_list.append(statistics.mean(cluster_wape_dict.values()))\n",
    "\n",
    "                mse_per_county_per_cluster_list.append(list(cluster_mse_dict.values()))\n",
    "                re_per_county_per_cluster_list.append(list(cluster_re_dict.values()))\n",
    "                rsquared_per_county_per_cluster_list.append(list(cluster_rsquared_dict.values()))\n",
    "                mape_per_county_per_cluster_list.append(list(cluster_mape_dict.values()))\n",
    "                wape_per_county_per_cluster_list.append(list(cluster_wape_dict.values()))\n",
    "\n",
    "        if cluster_mode == 'unclustered':\n",
    "            break  # only run once for unclustered dataset\n",
    "\n",
    "    return rmse_per_cluster_list, mse_per_county_per_cluster_list, mean_rsquared_per_cluster_list, rsquared_per_county_per_cluster_list, \\\n",
    "           mape_per_cluster_list, mape_per_county_per_cluster_list, wape_per_cluster_list, wape_per_county_per_cluster_list, \\\n",
    "           total_re_per_cluster_list, re_per_county_per_cluster_list, \\\n",
    "           unclustered_rmse_per_cluster_list, unclustered_mse_per_county_per_cluster_list, \\\n",
    "           unclustered_total_re_per_cluster_list, unclustered_re_per_county_per_cluster_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fitting the SEIR model to the data and estimating the parameters with the cluster id.\n",
    "    dataset_final_date = '8/1'\n",
    "    cluster_type = \"no_constants\"\n",
    "    run_mode = 'train'\n",
    "    # cluster_mode = 'clustered'\n",
    "\n",
    "    # initial_date_list = ['3/15', '4/1', '4/15', '5/1', '5/15']\n",
    "    # final_date_list = ['4/30', '5/15', '5/31', '6/15', '6/30']\n",
    "    # final_change_date_list = [datetime.datetime(2020, 4, 15), datetime.datetime(2020, 4, 30), datetime.datetime(2020, 5, 15), datetime.datetime(2020, 5, 31), datetime.datetime(2020, 6, 15)]\n",
    "\n",
    "    initial_date_list = ['5/1']\n",
    "    final_date_list = ['6/15']\n",
    "    final_change_date_list = [datetime.datetime(2020, 6, 5)]\n",
    "\n",
    "    # initial_date_list = ['4/1']\n",
    "    # final_date_list = ['5/15']\n",
    "    # final_change_date_list = [datetime.datetime(2020, 5, 5)]\n",
    "\n",
    "    for initial_date, final_date, final_change_date in zip(initial_date_list, final_date_list, final_change_date_list):\n",
    "        # load data\n",
    "        initial_clusters = getTzuHsiClusters(column_date=f\"{initial_date}~{final_date}\", cluster_type=cluster_type)\n",
    "        max_cluster_num = len(initial_clusters.unique())\n",
    "        cases_df = pd.read_csv(f'../../generated/us_cases_counties.csv', header=0, index_col=0)\n",
    "        vel_cases_df = pd.read_csv(f'../../generated/us_velocity_cases_counties.csv', header=0, index_col=0)\n",
    "        population_df = johnsHopkinsPopulation()\n",
    "\n",
    "        # set save path\n",
    "        date_info = f'{initial_date.replace(\"/\",\"-\")}_{final_date.replace(\"/\",\"-\")}_{dataset_final_date.replace(\"/\",\"-\")}_{final_change_date.strftime(\"%m-%d\")}_{cluster_type}'\n",
    "        root_save_path = f'../../generated/plots/{date_info}/'\n",
    "        if os.path.isdir(root_save_path) is False:\n",
    "            os.mkdir(root_save_path)\n",
    "\n",
    "        # initial Parameters\n",
    "        reassigned_clusters = initial_clusters\n",
    "        REASSIGN_COUNTER_MAX = 1\n",
    "        reassign_counter_init = 0  # to load cluster data from intermediate reassign num\n",
    "        init_cluster_num = 0\n",
    "\n",
    "        if run_mode == 'eval':\n",
    "            max_cluster_num = max_cluster_num - 1  # remove cluster 11 as a outlier\n",
    "\n",
    "        # if cluster_mode == 'unclustered':\n",
    "        #     REASSIGN_COUNTER_MAX = 1\n",
    "\n",
    "        # root_save_path = f'../../generated/plots/{date_info}/reassign_{reassign_counter_init-1}/'\n",
    "        # reassigned_clusters = pd.read_csv(root_save_path + f'clusters.csv', header=0, index_col=0)\n",
    "        # reassigned_clusters = reassigned_clusters.iloc[:, 0]\n",
    "        # reassigned_clusters = reassign_county(reassigned_clusters, max_cluster_num, cases_df, population_df, initial_date, final_date, num_days_future, root_save_path)\n",
    "\n",
    "        # -------------- Run Reassignment Model ------------------------\n",
    "        for reassign_counter in range(reassign_counter_init, REASSIGN_COUNTER_MAX):\n",
    "            print('reassign_counter: ', reassign_counter)\n",
    "            if reassign_counter < REASSIGN_COUNTER_MAX - 1:\n",
    "                num_days_future = 7  # only validation set\n",
    "            else:\n",
    "                num_days_future = 14  # validation + test set\n",
    "                \n",
    "            root_save_path = f'../../generated/plots/{date_info}/reassign_{reassign_counter}/'\n",
    "            if run_mode == 'eval':  # load reassigned data\n",
    "                reassigned_clusters = pd.read_csv(root_save_path + f'clusters.csv', header=0, index_col=0)\n",
    "                reassigned_clusters = reassigned_clusters.iloc[:, 0]\n",
    "            elif run_mode == 'train':\n",
    "                if os.path.isdir(root_save_path) is False:\n",
    "                    os.mkdir(root_save_path)\n",
    "                reassigned_clusters.to_csv(root_save_path + f'clusters.csv')  # save current county assignment to clusters\n",
    "            print(reassigned_clusters)\n",
    "\n",
    "            # ------------ Forecast -----------------\n",
    "            rmse_per_cluster_list, mse_per_county_per_cluster_list, mean_rsquared_per_cluster_list, rsquared_per_county_per_cluster_list, \\\n",
    "             mape_per_cluster_list, mape_per_county_per_cluster_list, wape_per_cluster_list, wape_per_county_per_cluster_list, \\\n",
    "            total_re_per_cluster_list, re_per_county_per_cluster_list, \\\n",
    "            _, _, _, _ = forecast_main(reassigned_clusters, cases_df, vel_cases_df, population_df, 'clustered',\n",
    "                                       init_cluster_num, max_cluster_num, initial_date, final_date, final_change_date,\n",
    "                                       num_days_future, dataset_final_date, run_mode, root_save_path)\n",
    "\n",
    "            _, _, _, _, \\\n",
    "            _, _, _, _, \\\n",
    "            _, _, \\\n",
    "            unclustered_rmse_per_cluster_list, unclustered_mse_per_county_per_cluster_list, \\\n",
    "            unclustered_total_re_per_cluster_list, unclustered_re_per_county_per_cluster_list \\\n",
    "                = forecast_main(reassigned_clusters, cases_df, vel_cases_df, population_df, 'unclustered',\n",
    "                                init_cluster_num, max_cluster_num, initial_date, final_date, final_change_date,\n",
    "                                num_days_future, dataset_final_date, run_mode, root_save_path)\n",
    "\n",
    "            # ------------------------------\n",
    "            if run_mode == 'train':\n",
    "                if reassign_counter < REASSIGN_COUNTER_MAX - 1:\n",
    "                    reassigned_clusters = reassign_county(reassigned_clusters, max_cluster_num, cases_df, vel_cases_df,\n",
    "                                                          population_df, initial_date, final_date,\n",
    "                                                          num_days_future, root_save_path)\n",
    "\n",
    "            # ------------ Evaluation ---------------\n",
    "            if run_mode == 'eval':\n",
    "\n",
    "                # ------------------ for clustered dataset -----------------------\n",
    "                clustered_average_mse = average_from_list_of_list(mse_per_county_per_cluster_list)\n",
    "                clustered_average_rmse = round(math.sqrt(clustered_average_mse), 3)\n",
    "                average_of_rsquared = round(average_from_list_of_list(rsquared_per_county_per_cluster_list), 3)\n",
    "                average_of_mape = round(average_from_list_of_list(mape_per_county_per_cluster_list), 3)\n",
    "                average_of_wape = round(average_from_list_of_list(wape_per_county_per_cluster_list), 3)\n",
    "\n",
    "                histogram_clusters(reassigned_clusters, max_cluster_num, root_save_path)\n",
    "\n",
    "                violin_eval_clusters(mse_per_county_per_cluster_list, 'MSE', root_save_path)\n",
    "                violin_eval_clusters(rsquared_per_county_per_cluster_list, 'R^2', root_save_path)\n",
    "                violin_eval_clusters(mape_per_county_per_cluster_list, 'MAPE', root_save_path)\n",
    "                violin_eval_clusters(wape_per_county_per_cluster_list, 'WAPE', root_save_path)\n",
    "\n",
    "                bar_eval_clusters(max_cluster_num, rmse_per_cluster_list, clustered_average_rmse, 'RMSE', 'clustered',\n",
    "                                  cluster_type, root_save_path)\n",
    "                bar_eval_clusters(max_cluster_num, mean_rsquared_per_cluster_list, average_of_rsquared, 'R^2', 'clustered',\n",
    "                                  cluster_type, root_save_path)\n",
    "                # bar_eval_clusters(max_cluster_num, mape_per_cluster_list, average_of_mape, 'MAPE', 'clustered',\n",
    "                #                   cluster_type, root_save_path)\n",
    "                bar_eval_clusters(max_cluster_num, wape_per_cluster_list, average_of_wape, 'WAPE', 'clustered',\n",
    "                                  cluster_type, root_save_path)\n",
    "\n",
    "                # --------------- unclustered ---------------------\n",
    "                unclustered_average_mse = average_from_list_of_list(unclustered_mse_per_county_per_cluster_list)\n",
    "                unclustered_average_rmse = round(math.sqrt(unclustered_average_mse), 3)\n",
    "                bar_eval_clusters(max_cluster_num, unclustered_rmse_per_cluster_list, unclustered_average_rmse, 'RMSE', 'unclustered',\n",
    "                                  cluster_type, root_save_path)\n",
    "\n",
    "                # ---- clustered and unclustered -----\n",
    "                print('----------------')\n",
    "\n",
    "                clustered_average_re = average_from_list_of_list(re_per_county_per_cluster_list)\n",
    "                unclustered_average_re = average_from_list_of_list(unclustered_re_per_county_per_cluster_list)\n",
    "                clustered_average_re = round(clustered_average_re, 3)\n",
    "                unclustered_average_re = round(unclustered_average_re, 3)\n",
    "\n",
    "                bar_eval_clusters_compare(rmse_per_cluster_list, unclustered_rmse_per_cluster_list, clustered_average_rmse, unclustered_average_rmse, 'RMSE', root_save_path)\n",
    "                bar_eval_clusters_compare(total_re_per_cluster_list, unclustered_total_re_per_cluster_list, clustered_average_re, unclustered_average_re, 'Relative Error', root_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tensorflow-2",
   "language": "python",
   "name": "tensorflow-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
